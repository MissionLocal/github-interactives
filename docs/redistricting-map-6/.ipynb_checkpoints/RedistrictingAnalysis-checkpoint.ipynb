{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cdf6c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\willi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import altair as alt\n",
    "\n",
    "import geopandas as gpd\n",
    "from geopandas import GeoDataFrame\n",
    "import cenpy as cen\n",
    "import datetime as dt\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "#pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba84df3f",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "203039ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'CensusRaceData_cleaned.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14616/1058852113.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#import race data for tracts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#https://data.census.gov/cedsci/table?q=race\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdfRace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'CensusRaceData_cleaned.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#filter to just San Francisco\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'CensusRaceData_cleaned.csv'"
     ]
    }
   ],
   "source": [
    "#import race data for tracts\n",
    "#https://data.census.gov/cedsci/table?q=race\n",
    "dfRace = pd.read_csv('CensusRaceData_cleaned.csv')\n",
    "\n",
    "#filter to just San Francisco\n",
    "dfRace = dfRace[dfRace.tract.str.contains('San Francisco')]\n",
    "\n",
    "#create GEOID\n",
    "dfRace['GEOID'] = dfRace['id'].str[10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242321c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import census tracts\n",
    "#https://data.sfgov.org/Geographic-Locations-and-Boundaries/Census-2020-Tracts-for-San-Francisco/tmph-tgz9\n",
    "dfTracts = gpd.read_file('https://data.sfgov.org/api/geospatial/tmph-tgz9?method=export&format=Shapefile')\n",
    "\n",
    "#turn GEOID into object\n",
    "dfTracts['GEOID'] = dfTracts['geoid'].astype(str).str[1:]\n",
    "\n",
    "#remove unnecessary columns\n",
    "dfTracts = dfTracts[['namelsad','GEOID','geometry']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3b4e91",
   "metadata": {},
   "source": [
    "### filter race data into neat columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b481baa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include everyone with every element \n",
    "dfRace['nonh_some_white'] = dfRace.loc[:,dfRace.columns.str.contains('white')].sum(axis=1)\n",
    "dfRace['nonh_some_black'] = dfRace.loc[:,dfRace.columns.str.contains('black')].sum(axis=1)\n",
    "dfRace['nonh_some_native'] = dfRace.loc[:,dfRace.columns.str.contains('native')].sum(axis=1)\n",
    "dfRace['nonh_some_asian'] = dfRace.loc[:,dfRace.columns.str.contains('asian')].sum(axis=1)\n",
    "dfRace['nonh_some_hawaiianorpacificislander'] = dfRace.loc[:,dfRace.columns.str.contains('hawaiianorpacificislander')].sum(axis=1)\n",
    "dfRace['nonh_some_other'] = dfRace.loc[:,dfRace.columns.str.contains('other')].sum(axis=1)\n",
    "\n",
    "#hone down to mixed race categories\n",
    "dfRaceAll = dfRace[['GEOID','total_pop','hispanic','nonh_some_white','nonh_some_black','nonh_some_native','nonh_some_asian','nonh_some_hawaiianorpacificislander','nonh_some_other']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1354e070",
   "metadata": {},
   "source": [
    "### merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc0ef3e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#merge\n",
    "dfRaceTracts = dfTracts.merge(dfRaceAll, on='GEOID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e848449",
   "metadata": {},
   "source": [
    "### chop out water/clean up geometries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c912ff",
   "metadata": {},
   "source": [
    "#### load current supervisor map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd386271",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load current supervisor map\n",
    "dfCurrentMap = gpd.read_file('s7rh6q.shp')\n",
    "\n",
    "#get rid of unnecessary columns\n",
    "dfCurrentMap = dfCurrentMap[['supervisor','geometry']]\n",
    "\n",
    "#set correct crs\n",
    "dfCurrentMap = dfCurrentMap.to_crs(4269)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b59ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#explode!\n",
    "dfCurrentMap = dfCurrentMap.explode().reset_index().drop(['level_0','level_1'],axis=1)\n",
    "\n",
    "#drop annoying sliver\n",
    "dfCurrentMap = dfCurrentMap.drop(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104fac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export as geoJSON\n",
    "dfCurrentMap.to_file(\"CurrentSupMap.geojson\", driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6135325c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create beautiful intersection map\n",
    "dfCurrentMapCity = dfCurrentMap.dissolve()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0558f6",
   "metadata": {},
   "source": [
    "#### cut everything down to current sup map size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7848e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rid of tracts without any population\n",
    "dfRaceTracts = dfRaceTracts[dfRaceTracts['total_pop'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1034a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chop off all tracts not within the supervisor map\n",
    "dfRaceTracts = dfRaceTracts.overlay(dfCurrentMapCity, how='intersection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7e0664",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rid of multipolygons\n",
    "dfRaceTracts = dfRaceTracts.explode().reset_index().drop(['level_0','level_1'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788cedeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot map\n",
    "alt.Chart(dfRaceTracts).mark_geoshape(\n",
    "    fill='lightgray',\n",
    "    stroke='darkgray',\n",
    "    strokeWidth=0.5\n",
    ").properties(\n",
    "    width=700,\n",
    "    height=350\n",
    ").encode(\n",
    "    tooltip='supervisor'\n",
    ").configure_view(strokeWidth=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6ddd43",
   "metadata": {},
   "source": [
    "### bin and clean and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6717e4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make mini geoJSON files\n",
    "gdfHispanic = dfRaceTracts[['namelsad','total_pop','hispanic','geometry']]\n",
    "gdfWhite = dfRaceTracts[['namelsad','total_pop','nonh_some_white','geometry']]\n",
    "gdfBlack = dfRaceTracts[['namelsad','total_pop','nonh_some_black','geometry']]\n",
    "gdfNative = dfRaceTracts[['namelsad','total_pop','nonh_some_native','geometry']]\n",
    "gdfAsian = dfRaceTracts[['namelsad','total_pop','nonh_some_asian','geometry']]\n",
    "gdfHawaii = dfRaceTracts[['namelsad','total_pop','nonh_some_hawaiianorpacificislander','geometry']]\n",
    "gdfOther = dfRaceTracts[['namelsad','total_pop','nonh_some_other','geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73f8cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#figure out population proportions\n",
    "gdfHispanic['population_prop'] = round(gdfHispanic['hispanic'] / gdfHispanic['total_pop'] * 100, 1)\n",
    "gdfWhite['population_prop'] = round(gdfWhite['nonh_some_white'] / gdfWhite['total_pop'] * 100, 1)\n",
    "gdfBlack['population_prop'] = round(gdfBlack['nonh_some_black'] / gdfBlack['total_pop'] * 100, 1)\n",
    "gdfNative['population_prop'] = round(gdfNative['nonh_some_native'] / gdfNative['total_pop'] * 100, 1)\n",
    "gdfAsian['population_prop'] = round(gdfAsian['nonh_some_asian'] / gdfAsian['total_pop'] * 100, 1)\n",
    "gdfHawaii['population_prop'] = round(gdfHawaii['nonh_some_hawaiianorpacificislander'] / gdfHawaii['total_pop'] * 100, 1)\n",
    "gdfOther['population_prop'] = round(gdfOther['nonh_some_other'] / gdfOther['total_pop'] * 100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606e59ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only doing four racial categories for now\n",
    "#bin population proportions\n",
    "gdfHispanic['population_prop_bin'] = pd.cut(gdfHispanic['population_prop'], [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100], labels=['0-10%','10-20%','20-30%','30-40%','40-50%','50-60%','60-70%','70-80%','80-90%','90-100%'])\n",
    "gdfWhite['population_prop_bin'] = pd.cut(gdfWhite['population_prop'], [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100], labels=['0-10%','10-20%','20-30%','30-40%','40-50%','50-60%','60-70%','70-80%','80-90%','90-100%'])\n",
    "gdfBlack['population_prop_bin'] = pd.cut(gdfBlack['population_prop'], [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100], labels=['0-10%','10-20%','20-30%','30-40%','40-50%','50-60%','60-70%','70-80%','80-90%','90-100%'])\n",
    "gdfAsian['population_prop_bin'] = pd.cut(gdfAsian['population_prop'], [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100], labels=['0-10%','10-20%','20-30%','30-40%','40-50%','50-60%','60-70%','70-80%','80-90%','90-100%'])\n",
    "\n",
    "#as strings\n",
    "gdfHispanic['population_prop_bin'] = gdfHispanic['population_prop_bin'].astype(str)\n",
    "gdfWhite['population_prop_bin'] = gdfWhite['population_prop_bin'].astype(str)\n",
    "gdfBlack['population_prop_bin'] = gdfBlack['population_prop_bin'].astype(str)\n",
    "gdfAsian['population_prop_bin'] = gdfAsian['population_prop_bin'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0604d680",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdfHispanic.to_file(\"gdfHispanic.geojson\", driver=\"GeoJSON\")\n",
    "gdfWhite.to_file(\"gdfWhite.geojson\", driver=\"GeoJSON\")\n",
    "gdfBlack.to_file(\"gdfBlack.geojson\", driver=\"GeoJSON\")\n",
    "gdfAsian.to_file(\"gdfAsian.geojson\", driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda72113",
   "metadata": {},
   "source": [
    "### load Map 4A shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6043e8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load District Map 4A\n",
    "#https://sf.gov/resource/2022/shapefiles-redistricting-task-force\n",
    "dfMap4A = gpd.read_file('Map_4_A 2022-03-26.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2df161",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rid of excess columns\n",
    "dfMap4A = dfMap4A[['DISTRICT','POPULATION','geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75612a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chop off all tracts not within the supervisor map\n",
    "dfMap4A = dfMap4A.overlay(dfCurrentMapCity, how='intersection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d266445",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rid of multipolygons\n",
    "dfMap4A = dfMap4A.explode().reset_index().drop(['level_0','level_1'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d016184a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot map\n",
    "alt.Chart(dfMap4A).mark_geoshape(\n",
    "    fill='lightgray',\n",
    "    stroke='darkgray',\n",
    "    strokeWidth=0.5\n",
    ").properties(\n",
    "    width=700,\n",
    "    height=350\n",
    ").configure_view(strokeWidth=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a99a894",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export as geoJSON\n",
    "dfMap4A.to_file(\"map4A.geojson\", driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1948630",
   "metadata": {},
   "source": [
    "#### create Map 4A labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474dc4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create label points dataframe\n",
    "dfMap4ALabels = [{'name': '1', 'latitude': 37.77913997009093, 'longitude': -122.48323943404117},\n",
    "                {'name': '2', 'latitude': 37.79379810348995, 'longitude': -122.45129308342577},\n",
    "                {'name': '3', 'latitude': 37.798289633908375, 'longitude': -122.41215762996562},\n",
    "                {'name': '4', 'latitude': 37.75131347352852, 'longitude': -122.49353420763208},\n",
    "                {'name': '5', 'latitude': 37.781415263711864, 'longitude': -122.42556748889184},\n",
    "                {'name': '6', 'latitude': 37.77909339877755, 'longitude': -122.39962883000551},\n",
    "                {'name': '7', 'latitude': 37.74303427929667, 'longitude': -122.45942975504916},\n",
    "                {'name': '8', 'latitude': 37.754962290530614, 'longitude': -122.43456520332234},\n",
    "                {'name': '9', 'latitude': 37.74643115886766, 'longitude': -122.41240960359413},\n",
    "                {'name': '10', 'latitude': 37.73469372001913, 'longitude': -122.38781089319022},\n",
    "                {'name': '11', 'latitude': 37.718526283728515, 'longitude': -122.44324447517572}]\n",
    "\n",
    "dfMap4ALabels = pd.DataFrame(dfMap4ALabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970cd539",
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn into geodataframe\n",
    "gdfMap4ALabels = gpd.GeoDataFrame(dfMap4ALabels, geometry=gpd.points_from_xy(dfMap4ALabels.longitude, dfMap4ALabels.latitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b0435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rid of unnecessary columns\n",
    "gdfMap4ALabels = gdfMap4ALabels[['name','geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13b6dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export as geoJSON\n",
    "gdfMap4ALabels.to_file(\"map4ALabels.geojson\", driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492091d5",
   "metadata": {},
   "source": [
    "### create median income tracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688267b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in data\n",
    "dfIncome = pd.read_csv('median_household_income_acs.csv')\n",
    "\n",
    "#create GEOID\n",
    "dfIncome['GEOID'] = dfIncome['id'].str[10:]\n",
    "\n",
    "#merge\n",
    "dfIncomeTracts = dfTracts.merge(dfIncome, on='GEOID')\n",
    "\n",
    "#drop unnecessary columns\n",
    "dfIncomeTracts = dfIncomeTracts[['namelsad','GEOID','estimated_household_income','estimated_household_income_moe','geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13ad0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chop off all tracts not within the supervisor map\n",
    "dfIncomeTracts = dfIncomeTracts.overlay(dfCurrentMapCity, how='intersection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81592dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rid of multipolygons\n",
    "dfIncomeTracts = dfIncomeTracts.explode().reset_index().drop(['level_0','level_1'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f26f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bin incomes\n",
    "dfIncomeTracts['income_bin'] = pd.cut(dfIncomeTracts['estimated_household_income'], [0, 20000, 40000, 60000, 80000, 100000, 120000, 140000, 160000, 180000, 200000, 1000000], labels=['0-20000','20000-40000','40000-60000','60000-80000','80000-100000','100000-120000','120000-140000','140000-160000','160000-180000','180000-200000','200000+'])\n",
    "\n",
    "#as strings\n",
    "dfIncomeTracts['income_bin'] = dfIncomeTracts['income_bin'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b8f9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot map\n",
    "alt.Chart(dfIncomeTracts).mark_geoshape(\n",
    "    stroke='darkgray',\n",
    "    strokeWidth=0.5\n",
    ").properties(\n",
    "    width=700,\n",
    "    height=350\n",
    ").encode(\n",
    "    tooltip='namelsad',\n",
    "    color='estimated_household_income:Q'\n",
    ").configure_view(strokeWidth=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6565bf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export as geoJSON\n",
    "dfIncomeTracts.to_file(\"dfIncomeTracts.geojson\", driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956e01b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
